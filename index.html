<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Privacy Preserving Machine Learning
    (ACM CCS 2021 Workshop)</title>
    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="js/mathjax/es5/tex-mml-chtml.js"></script>
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">PPML'21</span>
                </a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">Scope</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">CFP &amp; Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#grants">Grants</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Invited Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <!--<li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>-->
                    <li>
                        <a class="page-scroll" href="#organizers">Organization</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#previous">Previous Editions</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Privacy Preserving Machine Learning</h1>
                        <p class="intro-text">
                            Virtual <a href="https://www.sigsac.org/ccs/CCS2021/">ACM CCS 2021</a> Workshop
                            <br />
                            November 19, 2021
                        </p>
                        <!--<p class="location-text">
                            Palais des Congrès de Montréal
                            <br /> Room: 512CDGH
                        </p>-->
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Scope</h2>
                <p>This one day workshop focuses on privacy preserving techniques for training, inference, and disclosure in large scale data analysis, both in the distributed and centralized settings. We have observed increasing interest of the Machine Learning (ML) community in leveraging cryptographic techniques such as Multi-Party Computation (MPC) and Homomorphic Encryption (HE) for privacy preserving training and inference, as well as Differential Privacy (DP) for disclosure. Simultaneously, the systems security and cryptography community has proposed various secure frameworks for ML. We encourage both theory and application-oriented submissions exploring a range of approaches, including</p>
                <ul class="list-group">
                    <li class="list-group-item speaker">Differential privacy and other statistical notions of privacy: theory, applications, and implementations</li>
                    <li class="list-group-item speaker">Secure multi-party computation techniques for ML</li>
                    <li class="list-group-item speaker">Learning on encrypted data</li>
                    <li class="list-group-item speaker">Hardware-based approaches to privacy-preserving ML</li>
                    <li class="list-group-item speaker">Trade-offs between privacy and utility</li>
                    <li class="list-group-item speaker">Privacy attacks</li>
                    <li class="list-group-item speaker">Federated and decentralized privacy-preserving algorithms</li>
                    <li class="list-group-item speaker">Programming languages for privacy-preserving data analysis</li>
                    <li class="list-group-item speaker">Policy-making aspects of data privacy</li>
                    <li class="list-group-item speaker">Interplay between privacy and adversarial robustness in machine learning</li>
                    <li class="list-group-item speaker">Relations between privacy, fairness and transparency</li>
                    <li class="list-group-item speaker">Applications of privacy-preserving ML</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- CFP & Dates Section -->
    <section id="dates" class="container content-section text-center">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
          <h2>Call For Papers &amp; Important Dates</h2>
          <a href="cfp-ppml21.txt" class="btn btn-default btn-lg">Download Full CFP</a>
          <br />
          <br />
          <br />
          <p>
                    <b>Submission deadline</b>: <s>July 22</s> August 1, 2021, 23:59 (Anywhere on Earth)
                    <br /><b>Notification of acceptance</b>: September 16, 2021
                    <br /><b>Workshop</b>: November 19, 2021
                    <!-- <br /><b>CCS early <a -->
                    <!--                      href="https://www.sigsac.org/ccs/CCS2019/index.php/attending/registration/">registration</a> -->
                    <!--   deadline</b>: October 1, 2019 (11:59PM BST) -->
                    <!-- <br /><b>Workshop</b>: November 15, 2019 -->
          </p>
          <h3>Submission Instructions</h3>
          <p>
            Submissions in the form of extended abstracts must be at most 4 pages long (not including references), using the <a href="https://www.acm.org/publications/proceedings-template">double-column ACM format</a>.
            We encourage submission of work that is new to the privacy-preserving machine learning community.
            Submissions should be anonymized.
            The workshop will not have formal proceedings, but authors of accepted abstracts can choose to have a link to a preprint or a PDF published on the workshop webpage. Authors of accepted papers are required to register for the workshop but can present their work remotely.
          </p>

          <a href="https://ppml21.hotcrp.com" class="btn btn-default btn-lg">Submit Your  Abstract</a>
        </div>
      </div>
    </section>

    <!-- Call for travel grants -->
    <section id="grants" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Workshop Grants</h2>
                <p>
                    Thanks to our generous sponsors, we are able to provide a limited number of grants to cover the workshop registration fees of PPML attendees who have not received other support from CCS this year.
                    To apply, please send an email to <a
                        href="mailto:ppml2021@googlegroups.com?Subject=PPML21%20Grant%20Application">ppml2021@googlegroups.com</a>
                    with the subject “PPML21 Grant Application” including a half-page statement of purpose.
                    Please create an account in the <a href="https://genimice.com/ccs2021/index.do">CCS online registration system</a>, and <b>include your user ID</b> / email address in the application.
                    The deadline for applications is <b>November 5, 2021 (11:59pm AoE)</b>. The notifications
                    will be sent by <b>November 12</b>. Please feel free to send us an email if you have any questions.
                </p>
            </div>
        </div>
    </section>

    <!-- Speakers Section -->

    <section id="speakers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Invited Speakers</h2>
                <ul class="list-group">
                  <li class="list-group-item speaker"><a href="https://lsts.research.vub.be/en/mireille-hildebrandt">Mireille Hildebrandt</a> (Vrije Universiteit Brussel)</li>
                  <li class="list-group-item speaker"><a href="https://www.korolova.com/">Aleksandra Korolova</a> (University of Southern California)</li>
                  <li class="list-group-item speaker"><a href="https://ai.facebook.com/people/ilya-mironov/">Ilya Mironov</a> (Facebook AI)</li>
                  <li class="list-group-item speaker"><a href="http://www.pinkas.net/">Benny Pinkas</a> (Bar Ilan University)</li>
                </ul>
            </div>
        </div>
    </section>

<!-- Schedule Section -->
<section id="schedule" class="container content-section text-center">
    <div class="row">
        <div class="col-sm-8 col-sm-offset-2">
            <h2>Schedule</h2>
            <h3>Time Zone Accommodation</h3>
            <p>
            The workshop will be hosted in two blocks: <b>BLOCK I</b> accommodates Asia and Europe (morning) time zones, <b>BLOCK II</b> accommodates U.S. and Europe (evening) time zones. Unless otherwise noted, <b>all listed times are CET (UTC+1)</b>.
            </p>
            <table class="table schedule">
                <tbody>
                     <!-- Block slot -->
                     <tr>
                        <td colspan="2" class="block">BLOCK I, Asia/Europe </td>
                    </tr>
                    <!-- Basic slot -->
                    <tr>
                        <td class="time">9:10–9:20</td>
                        <td class="slot">Welcome & Introduction</td>
                    </tr>
                    <!-- Invited slot -->
                    <tr>
                        <td class="time">9:20–10:00</td>
                        <td class="slot talk">
                            Invited talk (1):
                            <a href="#tabs1" data-toggle="collapse" class="accordion-toggle">
                                Mireille Hildebrandt
                                &mdash;
                                PPML and the AI Act's Fundamental Rights Impact Assessment (FRIA) for ML Systems
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract"  id="tabs1">
                                Abstract<br />
                                Bringing together the ML community with MPC, HE and DP communities should allow for pivotal awareness within the ML community of myriad security and privacy issues that may affect both the reliability of ML systems and the confidentiality of the information these systems process. In this talk I will discuss how reducing access to identifiable information may nevertheless increase the risk to other fundamental rights, notably those of non-discrimination, presumption of innocence and freedom of information. This will be followed by an analysis of the legal requirement of a fundamental rights impact assessment (FRIA), referring to the EU’s GDPR and the EU’s proposed AI Act.
                                <br /><br />
                                Speaker Bio:<br />
                                Hildebrandt is a Research Professor on ‘Interfacing Law and Technology’ at Vrije Universiteit Brussels (VUB), appointed by the VUB Research Council. She is co-Director of the Research Group on Law Science Technology and Society studies (LSTS) at the Faculty of Law and Criminology.
                                She also holds the part-time Chair of Smart Environments, Data Protection and the Rule of Law at the Science Faculty, at the Institute for Computing and Information Sciences (iCIS) at Radboud University Nijmegen.
                                ​Her research interests concern the implications of automated decisions, machine learning and mindless artificial agency for law and the rule of law in constitutional democracies.
                                ​Hildebrandt has published 5 scientific monographs, 23 edited volumes or special issues, and well over 100 chapters and articles in scientific journals and volumes.
                                She received an ERC Advanced Grant for her project on ‘Counting as a Human Being in the era of Computational Law’ (2019-2024), that funds COHUBICOL. In that context she is co-founder of the international peer reviewed Journal of Cross-Disciplinary Research in Computational Law, together with Laurence Diver (co-Editors in Chief are Virginia Dignum and Frank Pasquale).
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">10:00–10:20</td>
                        <td class="slot talk"><a href="#tabs2" data-toggle="collapse" class="accordion-toggle">
                            Interaction data are identifiable even across long periods of time
                            </a>
                            <br />
                            <span style="font-weight: normal">
                                Ana-Maria Cretu (Imperial College London); Federico Monti (Twitter); Stefano Marrone (University of Naples Federico II); Xiaowen Dong (University of Oxford); Michael Bronstein, Yves-Alexandre de Montjoye (Imperial College London)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs2">
                                Fine-grained records of people's interactions, both offline and online, are collected at a large scale. These data contain sensitive information about whom we meet, talk to, and when. We demonstrate here how people's interaction behavior is stable over long periods of time and can be used to identify individuals in anonymous datasets. Our attack learns the profile of an individual using geometric deep learning and triplet loss optimization. In a mobile phone metadata dataset of more than 40k people, it correctly identifies 52% of individuals based on their 2-hop interaction graph. We further show that the profiles learned by our method are stable over time and that 24% of people are still identifiable after 20 weeks, thus making identification a real risk in practice. Finally, we show that having access to more auxiliary data can improve the performance of the attack, albeit with decreasing returns. Our results provide strong evidence that disconnected and even re-pseudonymized interaction data can be linked together making them likely to be personal data under the European Union's General Data Protection Regulation.
                            </div>
                        </td>
                    </tr>

                    <!-- Break slot -->
                    <tr>
                        <td class="time">10:20–10:30</td>
                        <td class="break">Break 10min</td>
                    </tr>

                    <!-- Invited slot -->
                    <tr>
                        <td class="time">10:30–11:10</td>
                        <td class="slot talk">
                            Invited talk (2):
                            <a href="#tabs3" data-toggle="collapse" class="accordion-toggle">
                                Benny Pinkas
                                &mdash;
                                TBD
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract"  id="tabs3">
                                TBD
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">11:10–11:30</td>
                        <td class="slot talk"><a href="#tabs4" data-toggle="collapse" class="accordion-toggle">
                            SIRNN: A Math Library for Secure RNN Inference</a>
                            <br />
                            <span style="font-weight: normal">
                                Deevashwer Rathee, Mayank Rathee (UC Berkeley); Rahul Kranti Kiran Goli, Divya Gupta, Rahul Sharma, Nishanth Chandran, Aseem Rastogi (Microsoft Research)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs4">
                              Complex machine learning (ML) inference algorithms like recurrent neural networks (RNNs) use standard functions from math libraries like exponentiation, sigmoid, tanh, and reciprocal of square root. Although prior work on secure 2-party inference provides specialized protocols for convolutional neural networks (CNNs), existing secure implementations of these math operators rely on generic 2-party computation (2PC) protocols that suffer from high communication. We provide new specialized 2PC protocols for math functions that crucially rely on lookup-tables and mixed- bitwidths to address this performance overhead; our protocols for math functions communicate up to 423× less data than prior work. Furthermore, our math implementations are numerically precise, which ensures that the secure implementations preserve model accuracy of cleartext. We build on top of our novel protocols to build SiRnn, a library for end-to-end secure 2-party DNN inference, that provides the first secure implementations of an RNN operating on time series sensor data, an RNN operating on speech data, and a state-of-the-art ML architecture that combines CNNs and RNNs for identifying all heads present in images. Our evaluation shows that SiRnn achieves up to three orders of magnitude of performance improvement when compared to inference of these models using an existing state-of-the-art 2PC framework.
                            </div>
                        </td>
                    </tr>

                    <!-- Break slot -->
                    <tr>
                        <td class="time">11:30–11:35</td>
                        <td class="break">Move to Gather</td>
                    </tr>

                    <!-- Basic slot -->
                    <tr>
                        <td class="time">11:35–12:35</td>
                        <td class="slot">Poster Session</td>
                    </tr>


                    <!-- Block slot -->
                    <tr>
                        <td class="time"> </td>
                        <td class="block">BLOCK II, Europe/US</td>
                    </tr>

                    <!-- Basic slot -->
                    <tr>
                        <td class="time">17:30–17:40</td>
                        <td class="slot">Welcome (back)</td>
                    </tr>

                    <!-- Invited slot -->
                    <tr>
                        <td class="time">17:40–18:20</td>
                        <td class="slot talk">
                            Invited talk (3):
                            <a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                                Ilya Mironov
                                &mdash;
                                Federated Learning: To TEE or Not to TEE?
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract"  id="tabs5">
                                Abstract:<br />
                                Cross-device Federated Learning (FL) is a distributed learning paradigm that promises to train high-quality models by leveraging data from massive client populations, while ensuring security and privacy of client data. A key component of FL protocols is secure aggregation of clients' updates, which can be implemented either by using traditional MPC techniques or by shifting some processing to a hardware-backed Trusted Execution Environment (TEE). We will discuss both approaches and their implications for supporting Internet-scale FL deployments.
                                <br /><br />
                                Speaker Bio:<br />
                                Ilya Mironov obtained his Ph.D. in cryptography from Stanford in 2003. In 2003-2014 he was a member of Microsoft Research (Silicon Valley Campus), where he contributed to early works on differential privacy. In 2015-2019 he worked in Google Brain. Since 2019 he has been part of Responsible AI (Meta Platforms, company previously known as Facebook) working on privacy-preserving machine learning.
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">18:20–18:40</td>
                        <td class="slot talk"><a href="#tabs6" data-toggle="collapse" class="accordion-toggle">
                            Canonical Noise and Private Hypothesis Tests</a>
                            <br />
                            <span style="font-weight: normal">
                                Jordan Awan (Purdue University); Salil Vadhan (Harvard University)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs6">
                                In the setting of $f$-DP, we propose the concept <i>canonical noise distribution</i> (CND) which captures whether an additive privacy mechanism is tailored for a given $f$, and give a construction of a CND for an arbitrary tradeoff function $f$. We show that private hypothesis tests are intimately related to CNDs, allowing for the release of private $p$-values at no additional privacy cost as well as the construction of uniformly most powerful (UMP) tests for binary data. We apply our techniques to difference of proportions testing.
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">18:40–19:20</td>
                        <td class="slot talk"><a href="#tabs7" data-toggle="collapse" class="accordion-toggle">
                            The Skellam Mechanism for Differentially Private Federated Learning</a>
                            <br />
                            <span style="font-weight: normal">
                                Naman Agarwal, Peter Kairouz, Ziyu Liu (Google Research)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs7">
                                We introduce the multi-dimensional Skellam mechanism, a discrete differential privacy mechanism based on the difference of two independent Poisson random variables. To quantify its privacy guarantees, we analyze the privacy loss distribution via a numerical evaluation and provide a sharp bound on the Rényi divergence between two shifted Skellam distributions. While useful in both centralized and distributed privacy applications, we investigate how it can be applied in the context of federated learning with secure aggregation under communication constraints. Our theoretical findings and extensive experimental evaluations demonstrate that the Skellam mechanism provides the same privacy-accuracy trade-offs as the continuous Gaussian mechanism, even when the precision is low. More importantly, Skellam is closed under summation and sampling from it only requires sampling from Poisson – an efficient routine that ships with all machine learning and data analysis software packages. These features, along with its discrete nature and competitive privacy-accuracy trade-offs, make it an attractive alternative to the newly introduced discrete Gaussian mechanism.
                            </div>
                        </td>
                    </tr>

                    <!-- Break slot -->
                    <tr>
                        <td class="time">19:20–19:30</td>
                        <td class="break">Break 10min</td>
                    </tr>

                    <!-- Invited slot -->
                    <tr>
                        <td class="time">19:30–20:10</td>
                        <td class="slot talk">
                            Invited talk (4):
                            <a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                                Aleksandra Korolova
                                &mdash;
                                Auditing the Hidden Societal Impacts of Ad Delivery Algorithms, with Implications to Privacy
                            </a>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract"  id="tabs8">
                                Abstract:<br />
                                Although targeted advertising has been touted as a way to give advertisers a choice in who they reach, increasingly, ad delivery algorithms designed by the ad platforms are invisibly refining those choices. In this talk, I will present our methodology for "black-box" auditing of the role of ad delivery algorithms in shaping who sees job and political ads using only the tools and data accessible to any advertiser. I will demonstrate that a platform's algorithmic choices can lead to skew in delivery of job ads along gender and racial lines, even when such skew was not intended by the advertiser and is not justified by differences in qualifications. Furthermore, I will show that a platform's choices shape the political ad delivery by hindering campaigns' ability to reach ideologically diverse voters. I will conclude by discussing the implications of our findings for the necessity of third-party auditing and the open questions of how to enable such auditing while preserving privacy.<br />
                                Based on joint work with Muhammad Ali, Miranda Bogen, John Heidemann, Basileal Imana,  Alan Mislove, Aaron Rieke, Piotr Sapiezynski.
                                <br /><br />
                                Speaker Bio:<br />
                                Aleksandra Korolova is a WiSE Gabilan Assistant Professor of Computer Science at USC, where she studies societal impacts of algorithms and develops algorithms that enable data-driven innovations while preserving privacy and fairness. Prior to joining USC, Aleksandra was a research scientist at Google and a Computer Science Ph.D. student at Stanford. Aleksandra is a recipient of the 2020 NSF CAREER award, a co-winner of the 2011 PET Award for outstanding research in privacy enhancing technologies for exposing privacy violations of microtargeted advertising and a runner-up for the 2015 PET Award for RAPPOR, the first commercial deployment of differential privacy. Aleksandra's most recent research, on discrimination in ad delivery, has received the CSCW Honorable Mention Award and Recognition of Contribution to Diversity and Inclusion, was a runner-up for the WWW Best Student Paper Award, and was invited for a briefing for Members of the House Financial Services Committee.
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">20:10–20:30</td>
                        <td class="slot talk"><a href="#tabs9" data-toggle="collapse" class="accordion-toggle">
                            NeuraCrypt is not private</a>
                            <br />
                            <span style="font-weight: normal">
                                Nicholas Carlini (Google); Sanjam Garg (University of California, Berkeley and NTT Research); Somesh Jha (University of Wisconsin); Saeed Mahloujifar (Princeton); Mohammad Mahmoody (University of Virginia); Florian Tramèr (Stanford University)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs9">
                                NeuraCrypt (Yara et al. arXiv 2021) is an algorithm that converts a sensitive dataset to an encoded dataset so that (1) it is still possible to train machine learning models on the encoded data, but (2) an adversary who has access only to the encoded dataset can not learn much about the original sensitive dataset. We break NeuraCrypt's privacy claims, by perfectly solving the authors' public challenge, and by showing that NeuraCrypt does not satisfy the formal privacy definitions posed in the original paper. Our attack consists of a series of boosting steps that, coupled with various design flaws, turns a 1% attack advantage into a 100% complete break of the scheme.
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">20:30–20:50</td>
                        <td class="slot talk"><a href="#tabs10" data-toggle="collapse" class="accordion-toggle">
                            What else is leaked when eavesdropping federated learning?</a>
                            <br />
                            <span style="font-weight: normal">
                                Chuan Xu, Giovanni Neglia (Inria)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs10">
                                In this paper, we initiate the study of <i>local model reconstruction attacks</i> for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between the client and the server and reconstructs the local model of the client. The success of this attack enables better performance of other known attacks, such as the membership attack, attribute inference attacks, etc. We provide analytical guarantees for the success of this attack when training a linear least squares problem with full batch size and arbitrary number of local steps. One heuristic is proposed to generalize the attack to other machine learning problems. Experiments are conducted on logistic regression tasks, showing high reconstruction quality, especially when clients' datasets are highly heterogeneous (as it is common in federated learning)
                            </div>
                        </td>
                    </tr>

                    <!-- Contributed slot -->
                    <tr>
                        <td class="time">20:50–21:10</td>
                        <td class="slot talk"><a href="#tabs11" data-toggle="collapse" class="accordion-toggle">
                            FHE-Friendly Distillation of Decision Tree Ensembles for Efficient Encrypted Inference</a>
                            <br />
                            <span style="font-weight: normal">
                                Karthik Nandakumar (Mohamed Bin Zayed University of Artificial Intelligence); Kanthi Sarpatwar (IBM T. J. Watson Research Center); Nalini Ratha (University of Buffalo); Sharath Pankanti (Microsoft); Roman Vaculin, Karthikeyan Shanmugam, James T Rayfield (IBM Research)
                            </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs11">
                                Data privacy concerns often limit the use of cloud-based machine learning services for processing sensitive personal data. While fully homomorphic encryption (FHE) offers a potential solution by enabling computations on encrypted data, the challenge is to obtain accurate machine learning models that work efficiently within FHE limitations. Though deep neural networks have been very successful in many applications, decision tree ensembles are still considered the state-of-the-art for inference on tabular data. Existing approaches for encrypted inference based on decision trees simply replace hard comparison operations with soft comparators at the cost of accuracy. In this work, we propose a framework to distill knowledge extracted by decision tree ensembles to shallow neural networks (referred to as FDNets) that are highly conducive to encrypted inference. The proposed FDNets are FHE-friendly because they are obtained by searching for the best multilayer perceptron (MLP) architecture that minimizes the accuracy loss while operating within the given depth constraints. Furthermore, the FDNets can be trained using only synthetic data sampled from the training data distribution without the need for accessing the original training data. Extensive experiments on real-world datasets demonstrate that FDNets are highly scalable and can perform efficient inference on batched encrypted (134 bits of security) data with amortized time in milliseconds.
                            </div>
                        </td>
                    </tr>

                    <!-- Break slot -->
                    <tr>
                        <td class="time">21:10–21:15</td>
                        <td class="break">Move to Gather</td>
                    </tr>

                    <!-- Basic slot -->
                    <tr>
                        <td class="time">21:15–22:15</td>
                        <td class="slot">Poster Session</td>
                    </tr>

                </tbody>
            </table>
        </div>
    </div>
</section>

    <!-- Accepted Papers -->
<!--
    <section id="papers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Accepted Papers</h2>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            (Author names)
                        </span>

-->
                        <!-- Standard Template  -->
                        <!--
                        < a data-toggle="collapse" href="#abs0" class="paper-title">
                            Title
                        </a> &nbsp;&nbsp;
                        -->
                        <!-- Paper w/ PDF Template  -->
                        <!-- <a data-toggle="collapse" href="#abs0" class="paper-title"> -->
                            <!-- Title (paper with PDF) -->
                        <!-- </a> &nbsp;&nbsp; -->
                        <!-- <a href="pdfs/13.pdf" class="link-paper">[PDF]</a> -->
                        <!-- Paper w/ arXiv Link Template -->
                        <!-- <a data-toggle="collapse" href="#abs0" class="paper-title">
                        TITLE (paper with Arvix link)
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.11124" class="link-paper">[arxiv]</a> -->
                        <!-- Paper w/ Contributed Talk Template -->

<!--
                    <div id="abs0" class="panel-footer panel-paper-footer collapse">
                        Poster B1
                    </div>
                </div>
            </div>
        </div>
    </section>
-->


    <!-- Organizers Section -->
    <section id="organizers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Organization</h2>
                <br />
                <h3>Workshop organizers</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">James Bell (The Alan Turing Institute)</li>
                    <li class="list-group-item organizer">Aurélien Bellet (INRIA)</li>
                    <li class="list-group-item organizer">Adrià Gascón (Google)</li>
                    <li class="list-group-item organizer">Olya Ohrimenko (The University of Melbourne)</li>
                    <li class="list-group-item organizer">Mariana Raykova (Google)</li>
                    <li class="list-group-item organizer">Phillipp Schoppmann (Google)</li>
                    <li class="list-group-item organizer">Carmela Troncoso (EPFL)</li>
                </ul>
                <br />
                <h3>Program Committee</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">Carsten Baum (Aarhus University)</li>
                    <li class="list-group-item organizer">Hao Chen (Facebook)</li>
                    <li class="list-group-item organizer">Giovanni Cherubin (The Alan Turing Institute)</li>
                    <li class="list-group-item organizer">Albert Cheu (Northeastern)</li>
                    <li class="list-group-item organizer">Graham Cormode (University of Warwick)</li>
                    <li class="list-group-item organizer">Morten Dahl (Cape Privacy)</li>
                    <li class="list-group-item organizer">Daniel Demmler (Hamburg University)</li>
                    <li class="list-group-item organizer">Antti Honkela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Bargav Jayaraman (University of Virginia)</li>
                    <li class="list-group-item organizer">Dali Kaafar (Macquarie University)</li>
                    <li class="list-group-item organizer">Peter Kairouz (Google)</li>
                    <li class="list-group-item organizer">Marcel Keller (CSIRO's Data61)</li>
                    <li class="list-group-item organizer">Ágnes Kiss (CISPA Helmholtz Center)</li>
                    <li class="list-group-item organizer">Antti Koskela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Kim Laine (Microsoft Research)</li>
                    <li class="list-group-item organizer">Eleftheria Makri (KU Leuven)</li>
                    <li class="list-group-item organizer">Peihan Miao (University of Illinois at Chicago)</li>
                    <li class="list-group-item organizer">Catuscia Palamidessi (Ecole Polytechnique & Inria)</li>
                    <li class="list-group-item organizer">Rachel Player (Royal Holloway)</li>
                    <li class="list-group-item organizer">Ananth Ragunathan (Facebook)</li>
                    <li class="list-group-item organizer">Divya Ravi (Aarhus University)</li>
                    <li class="list-group-item organizer">Leonie Reichert (HU Berlin)</li>
                    <li class="list-group-item organizer">Peter Rindal (Visa Research)</li>
                    <li class="list-group-item organizer">Dragos Rotaru (Cape Privacy)</li>
                    <li class="list-group-item organizer">Prateek Saxena (National University of Singapore)</li>
                    <li class="list-group-item organizer">Peter Scholl (Aarhus University)</li>
                    <li class="list-group-item organizer">Reza Shokri (National University of Singapore)</li>
                    <li class="list-group-item organizer">Mark Simkin (Aarhus University)</li>
                    <li class="list-group-item organizer">Nigel Smart (KU Leuven)</li>
                    <li class="list-group-item organizer">Oleksandr Tkachenko (TU Darmstadt)</li>
                    <li class="list-group-item organizer">Juan Ramón Troncoso-Pastoriza (EPFL)</li>
                    <li class="list-group-item organizer">Jon Ullman (Northeastern)</li>
                    <li class="list-group-item organizer">Jalaj Upadhyay (Apple)</li>
                    <li class="list-group-item organizer">Sameer Wagh (Berkeley)</li>
                    <li class="list-group-item organizer">Xiao Wang (Northwestern University)</li>
                    <li class="list-group-item organizer">Christian Weinert (TU Darmstadt)</li>
                    <li class="list-group-item organizer">Sophia Yakoubov (Aarhus University)</li>
                    <li class="list-group-item organizer">Yang Zhang (CISPA Helmholtz Center)</li>
                </ul>
                <!--
                <br />
                <h3>Sponsors</h3>
                <br />
                <img style="margin:50px;" height="80" src="img/ati-white.png">
                <img style="margin:50px;" height="60" src="img/google.png">
                -->
            </div>
        </div>
    </section>

    <!-- Previous Editions -->
    <section id="previous" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Previous Editions</h2>
                <br />
                <ul class="list-group">
                  <li class="list-group-item event"><a href="/ppml18/index.html">PPML'18</a> @ NeurIPS</li>
                  <li class="list-group-item event"><a href="/ppml19/index.html">PPML'19</a> @ CCS</li>
                  <li class="list-group-item event"><a href="/ppml20/index.html">PPML'20</a> @ NeurIPS</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Contact us: <a href="mailto:ppml2021@googlegroups.com">ppml2021@googlegroups.com</a></p>
            <br />
        </div>
    </footer>
    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>
    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>
    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <!-- Theme JavaScript -->
    <script src="js/script.js"></script>
</body>

</html>
